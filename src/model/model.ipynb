{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: https://github.com/LeanManager/NLP-PyTorch/blob/master/Character-Level%20LSTM%20with%20PyTorch.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "from src.model.model import CharRNN\n",
    "from src.model.train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/corpus.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2char = json.load(open('../data/int2char.json', 'r'))\n",
    "char2int = json.load(open('../data/char2int.json', 'r'))\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(341, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=341, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialize and print the network\n",
    "net = CharRNN(list(char2int.keys()), n_hidden=512, n_layers=2)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.8582... Val Loss: 3.8077\n",
      "Epoch: 1/1... Step: 20... Loss: 3.6072... Val Loss: 3.5978\n",
      "Epoch: 1/1... Step: 30... Loss: 3.4646... Val Loss: 3.4591\n",
      "Epoch: 1/1... Step: 40... Loss: 3.3365... Val Loss: 3.3200\n",
      "Epoch: 1/1... Step: 50... Loss: 3.1476... Val Loss: 3.1476\n",
      "Epoch: 1/1... Step: 60... Loss: 2.9591... Val Loss: 2.9591\n",
      "Epoch: 1/1... Step: 70... Loss: 2.8081... Val Loss: 2.8158\n",
      "Epoch: 1/1... Step: 80... Loss: 2.7322... Val Loss: 2.7158\n",
      "Epoch: 1/1... Step: 90... Loss: 2.6639... Val Loss: 2.6325\n",
      "Epoch: 1/1... Step: 100... Loss: 2.5383... Val Loss: 2.5777\n",
      "Epoch: 1/1... Step: 110... Loss: 2.5192... Val Loss: 2.5420\n",
      "Epoch: 1/1... Step: 120... Loss: 2.4983... Val Loss: 2.5043\n",
      "Epoch: 1/1... Step: 130... Loss: 2.4546... Val Loss: 2.4687\n",
      "Epoch: 1/1... Step: 140... Loss: 2.4417... Val Loss: 2.4427\n",
      "Epoch: 1/1... Step: 150... Loss: 2.4227... Val Loss: 2.4161\n",
      "Epoch: 1/1... Step: 160... Loss: 2.4079... Val Loss: 2.3921\n",
      "Epoch: 1/1... Step: 170... Loss: 2.3714... Val Loss: 2.3679\n",
      "Epoch: 1/1... Step: 180... Loss: 2.3338... Val Loss: 2.3527\n",
      "Epoch: 1/1... Step: 190... Loss: 2.2971... Val Loss: 2.3394\n",
      "Epoch: 1/1... Step: 200... Loss: 2.2802... Val Loss: 2.3134\n",
      "Epoch: 1/1... Step: 210... Loss: 2.2710... Val Loss: 2.3014\n",
      "Epoch: 1/1... Step: 220... Loss: 2.2546... Val Loss: 2.2853\n",
      "Epoch: 1/1... Step: 230... Loss: 2.2317... Val Loss: 2.2653\n",
      "Epoch: 1/1... Step: 240... Loss: 2.2503... Val Loss: 2.2509\n",
      "Epoch: 1/1... Step: 250... Loss: 2.2035... Val Loss: 2.2367\n",
      "Epoch: 1/1... Step: 260... Loss: 2.1975... Val Loss: 2.2259\n",
      "Epoch: 1/1... Step: 270... Loss: 2.2069... Val Loss: 2.2132\n",
      "Epoch: 1/1... Step: 280... Loss: 2.1924... Val Loss: 2.2032\n",
      "Epoch: 1/1... Step: 290... Loss: 2.1782... Val Loss: 2.1886\n",
      "Epoch: 1/1... Step: 300... Loss: 2.1593... Val Loss: 2.1785\n",
      "Epoch: 1/1... Step: 310... Loss: 2.1397... Val Loss: 2.1666\n",
      "Epoch: 1/1... Step: 320... Loss: 2.1151... Val Loss: 2.1571\n",
      "Epoch: 1/1... Step: 330... Loss: 2.1094... Val Loss: 2.1475\n",
      "Epoch: 1/1... Step: 340... Loss: 2.1134... Val Loss: 2.1378\n",
      "Epoch: 1/1... Step: 350... Loss: 2.1305... Val Loss: 2.1284\n",
      "Epoch: 1/1... Step: 360... Loss: 2.0979... Val Loss: 2.1219\n",
      "Epoch: 1/1... Step: 370... Loss: 2.1117... Val Loss: 2.1125\n",
      "Epoch: 1/1... Step: 380... Loss: 2.0835... Val Loss: 2.1071\n",
      "Epoch: 1/1... Step: 390... Loss: 2.0412... Val Loss: 2.0943\n",
      "Epoch: 1/1... Step: 400... Loss: 2.0819... Val Loss: 2.0831\n",
      "Epoch: 1/1... Step: 410... Loss: 2.0353... Val Loss: 2.0751\n",
      "Epoch: 1/1... Step: 420... Loss: 2.0419... Val Loss: 2.0661\n",
      "Epoch: 1/1... Step: 430... Loss: 2.0361... Val Loss: 2.0623\n",
      "Epoch: 1/1... Step: 440... Loss: 2.0142... Val Loss: 2.0563\n",
      "Epoch: 1/1... Step: 450... Loss: 1.9955... Val Loss: 2.0456\n",
      "Epoch: 1/1... Step: 460... Loss: 2.0068... Val Loss: 2.0369\n",
      "Epoch: 1/1... Step: 470... Loss: 2.0275... Val Loss: 2.0288\n",
      "Epoch: 1/1... Step: 480... Loss: 1.9928... Val Loss: 2.0230\n",
      "Epoch: 1/1... Step: 490... Loss: 2.0103... Val Loss: 2.0162\n",
      "Epoch: 1/1... Step: 500... Loss: 1.9706... Val Loss: 2.0112\n",
      "Epoch: 1/1... Step: 510... Loss: 2.0020... Val Loss: 2.0019\n",
      "Epoch: 1/1... Step: 520... Loss: 1.9726... Val Loss: 1.9989\n",
      "Epoch: 1/1... Step: 530... Loss: 1.9891... Val Loss: 1.9919\n",
      "Epoch: 1/1... Step: 540... Loss: 1.9757... Val Loss: 1.9843\n",
      "Epoch: 1/1... Step: 550... Loss: 1.9237... Val Loss: 1.9777\n",
      "Epoch: 1/1... Step: 560... Loss: 1.9180... Val Loss: 1.9728\n",
      "Epoch: 1/1... Step: 570... Loss: 1.9222... Val Loss: 1.9679\n",
      "Epoch: 1/1... Step: 580... Loss: 1.9822... Val Loss: 1.9609\n",
      "Epoch: 1/1... Step: 590... Loss: 1.9310... Val Loss: 1.9554\n",
      "Epoch: 1/1... Step: 600... Loss: 1.8856... Val Loss: 1.9494\n",
      "Epoch: 1/1... Step: 610... Loss: 1.8647... Val Loss: 1.9470\n",
      "Epoch: 1/1... Step: 620... Loss: 1.9321... Val Loss: 1.9405\n",
      "Epoch: 1/1... Step: 630... Loss: 1.9099... Val Loss: 1.9333\n",
      "Epoch: 1/1... Step: 640... Loss: 1.8956... Val Loss: 1.9288\n",
      "Epoch: 1/1... Step: 650... Loss: 1.9353... Val Loss: 1.9234\n",
      "Epoch: 1/1... Step: 660... Loss: 1.8559... Val Loss: 1.9187\n",
      "Epoch: 1/1... Step: 670... Loss: 1.8999... Val Loss: 1.9131\n",
      "Epoch: 1/1... Step: 680... Loss: 1.8615... Val Loss: 1.9065\n",
      "Epoch: 1/1... Step: 690... Loss: 1.8617... Val Loss: 1.9032\n",
      "Epoch: 1/1... Step: 700... Loss: 1.8643... Val Loss: 1.8996\n",
      "Epoch: 1/1... Step: 710... Loss: 1.8702... Val Loss: 1.8927\n",
      "Epoch: 1/1... Step: 720... Loss: 1.8724... Val Loss: 1.8873\n",
      "Epoch: 1/1... Step: 730... Loss: 1.8598... Val Loss: 1.8821\n",
      "Epoch: 1/1... Step: 740... Loss: 1.8581... Val Loss: 1.8815\n",
      "Epoch: 1/1... Step: 750... Loss: 1.8338... Val Loss: 1.8793\n",
      "Epoch: 1/1... Step: 760... Loss: 1.8507... Val Loss: 1.8713\n",
      "Epoch: 1/1... Step: 770... Loss: 1.8215... Val Loss: 1.8672\n",
      "Epoch: 1/1... Step: 780... Loss: 1.8276... Val Loss: 1.8621\n",
      "Epoch: 1/1... Step: 790... Loss: 1.8217... Val Loss: 1.8579\n",
      "Epoch: 1/1... Step: 800... Loss: 1.8198... Val Loss: 1.8539\n",
      "Epoch: 1/1... Step: 810... Loss: 1.8034... Val Loss: 1.8488\n",
      "Epoch: 1/1... Step: 820... Loss: 1.8587... Val Loss: 1.8477\n",
      "Epoch: 1/1... Step: 830... Loss: 1.7808... Val Loss: 1.8458\n",
      "Epoch: 1/1... Step: 840... Loss: 1.8014... Val Loss: 1.8392\n",
      "Epoch: 1/1... Step: 850... Loss: 1.7907... Val Loss: 1.8341\n",
      "Epoch: 1/1... Step: 860... Loss: 1.8085... Val Loss: 1.8318\n",
      "Epoch: 1/1... Step: 870... Loss: 1.8083... Val Loss: 1.8265\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/arthur/Documents/german-anagram-solver/src/model/model.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/arthur/Documents/german-anagram-solver/src/model/model.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m n_seqs, n_steps \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m, \u001b[39m100\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/arthur/Documents/german-anagram-solver/src/model/model.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m train(net, encoded, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, n_seqs\u001b[39m=\u001b[39;49mn_seqs, n_steps\u001b[39m=\u001b[39;49mn_steps, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, cuda\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, print_every\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/german-anagram-solver/src/model/train.py:133\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, n_seqs, n_steps, lr, clip, val_frac, cuda, print_every)\u001b[0m\n\u001b[1;32m    130\u001b[0m     output, val_h \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mforward(inputs, val_h)\n\u001b[1;32m    131\u001b[0m     val_loss \u001b[39m=\u001b[39m criterion(output, targets\u001b[39m.\u001b[39mview(n_seqs\u001b[39m*\u001b[39mn_steps)\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mLongTensor))\n\u001b[0;32m--> 133\u001b[0m     val_losses\u001b[39m.\u001b[39mappend(val_loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m    135\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, epochs),\n\u001b[1;32m    136\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mStep: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(counter),\n\u001b[1;32m    137\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(loss\u001b[39m.\u001b[39mitem()),\n\u001b[1;32m    138\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mVal Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(np\u001b[39m.\u001b[39mmean(val_losses)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "\n",
    "train(net, encoded, epochs=1, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
